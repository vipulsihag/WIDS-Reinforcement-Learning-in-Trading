{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2c1ee1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Based Trading Agent \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006aba47",
   "metadata": {},
   "source": [
    "## 1. Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daf454",
   "metadata": {},
   "source": [
    "## 2. Download Market Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c1914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading days: 1511\n",
      "Sample price: 72.46826934814453 <class 'numpy.float64'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "symbol = \"AAPL\"\n",
    "\n",
    "data = yf.download(tickers= \"AAPL\",start=\"2020-01-01\",end= \"2026-01-07\")\n",
    "\n",
    "prices = data[\"Close\"].to_numpy().flatten()\n",
    "\n",
    "print(\"Trading days:\", len(prices))\n",
    "print(\"Sample price:\", prices[0], type(prices[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1d416",
   "metadata": {},
   "source": [
    "## 3. Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trading environment for Reinforcement Learning\n",
    "CAPITAL = 1000\n",
    "class TradingEnv:\n",
    "    def __init__(self, prices):\n",
    "        self.prices = prices\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.cash = CAPITAL\n",
    "        # 0 = no stock, 1 = holding stock\n",
    "        self.stock = False\n",
    "        self.done = False\n",
    "        \n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # State should contain:\n",
    "        # 1. current price\n",
    "        # 2. stock holding (0 or 1)\n",
    "        state = np.array([self.prices[self.t], self.stock],dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        price = self.prices[self.t]\n",
    "\n",
    "        # Action 0 → Hold (do nothing)\n",
    "        # Action 1 → Buy (only if enough cash)\n",
    "        # Action 2 → Sell (only if holding stock)\n",
    "        if action == 0:\n",
    "            pass\n",
    "        elif action == 1 and (not self.stock) and self.cash >= price:\n",
    "            self.stock = 1\n",
    "            self.cash -= price\n",
    "        elif action == 2 and self.stock:\n",
    "                self.cash += price\n",
    "                self.stock = False\n",
    "\n",
    "\n",
    "        # TODO: move to next time step\n",
    "        self.t += 1\n",
    "\n",
    "        # TODO: check termination condition\n",
    "        if self.t >= len(self.prices):\n",
    "             self.done = True\n",
    "             self.t = len(prices) - 1\n",
    "\n",
    "        # TODO: define reward (portfolio value)\n",
    "        reward = self.cash + self.stock*price\n",
    "\n",
    "        # TODO: return next_state, reward, done\n",
    "        return (self._get_state(), reward, self.done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7604ec94",
   "metadata": {},
   "source": [
    "## 4. Q-Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aedf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of states = number of time steps\n",
    "# number of actions = 3 (Hold, Buy, Sell)\n",
    "Q = np.zeros((len(prices), 3))\n",
    "\n",
    "# set learning rate (alpha)\n",
    "alpha = 0.1\n",
    "\n",
    "# set discount factor (gamma)\n",
    "gamma = 0.95\n",
    "\n",
    "# set exploration rate (epsilon)\n",
    "epsilon = 1\n",
    "decay = 0.9997\n",
    "\n",
    "# transaction fee (only to discourage agent from making toooo many trades, not really deducted)\n",
    "fee = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acdbd77",
   "metadata": {},
   "source": [
    "## 5. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024232dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Complete.\n",
      "Muhahahaha\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create trading environment\n",
    "env = TradingEnv(prices)\n",
    "\n",
    "# set number of training episodes\n",
    "episodes = 10000\n",
    "\n",
    "# training loop\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    # reset environment at start of each episode\n",
    "    state = env.reset()\n",
    "\n",
    "    # loop until episode ends\n",
    "    while not env.done:\n",
    "        \n",
    "        # get current state index (time step)\n",
    "        t = env.t\n",
    "\n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(0,3)\n",
    "        else:\n",
    "            action = np.argmax(Q[t])\n",
    "\n",
    "        # take action in environment\n",
    "        state, reward, done = env.step(action)\n",
    "\n",
    "        # update Q-value using Bellman equation\n",
    "        if env.done:\n",
    "            Q[t, action] += alpha * (reward - Q[t, action] -fee)\n",
    "        else:\n",
    "            Q[t, action] += alpha * (\n",
    "                reward +\n",
    "                gamma * np.max(Q[t + 1]) -\n",
    "                Q[t, action] - fee\n",
    "            )\n",
    "    epsilon = max(0.01,epsilon*decay)\n",
    "\n",
    "\n",
    "# indicate training completion\n",
    "print(\"Traning Complete.\\nMuhahahaha\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6365f3",
   "metadata": {},
   "source": [
    "## 6. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You now have : \n",
      "Wait for it\n",
      ".........\n",
      ".........\n",
      ".........\n",
      ".........\n",
      "\n",
      "$1255.7932815551758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a new environment for evaluation\n",
    "env = TradingEnv(prices)\n",
    "\n",
    "\n",
    "# run the trained agent without exploration\n",
    "while not env.done:\n",
    "    \n",
    "    # get current state index (time step)\n",
    "    t = env.t\n",
    "    \n",
    "    # select best action from Q-table\n",
    "    action = np.argmax(Q[t])\n",
    "    \n",
    "    # apply action in environment\n",
    "    env.step(action)\n",
    "\n",
    "# compute final portfolio value\n",
    "final_value = env.cash + env.stock*prices[-1]\n",
    "# print final result\n",
    "print(f\"You now have : \\nWait for it\\n.........\\n.........\\n.........\\n.........\\n\\n${final_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb64b5b",
   "metadata": {},
   "source": [
    "## 7. Buy and Hold Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You noob i earned \n",
      "\n",
      "1189.891716003418.\n"
     ]
    }
   ],
   "source": [
    "# implement Buy-and-Hold baseline strategy\n",
    "# - Buy one stock on the first day\n",
    "# - Hold it until the last day\n",
    "# - Start with initial cash of 1000\n",
    "\n",
    "buy_and_hold_value = 1000 - prices[0] + prices[-1]\n",
    "# print Buy-and-Hold portfolio value\n",
    "print(f\"You noob i earned \\n\\n{buy_and_hold_value}.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
